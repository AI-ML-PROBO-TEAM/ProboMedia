# Pegasus For summarisation

Pegasus is trained on a large-scale dataset that includes news articles from various sources. The dataset consists of pairs of source articles and human-written summaries. During training, the model learns to generate abstractive summaries that capture the key information and main ideas of the source article.

The training process involves fine-tuning the model using a variant of the Transformer architecture and employing techniques such as pretraining with denoising objectives and masked language modeling. By training on news data, Pegasus becomes proficient at summarizing news articles and understanding the structure, content, and nuances of news text.

The specific architecture used in Pegasus is similar to the Transformer architecture, which incorporates self-attention mechanisms and encoder-decoder layers. However, Pegasus introduces certain modifications and improvements to better handle the challenges of abstractive text summarization on news articles.

It is worth noting that while Pegasus is primarily trained on news data, the underlying architecture and techniques can also be applied to other domains and types of text data for summarization tasks. The general principles and capabilities of the model make it adaptable to various domains beyond news.

